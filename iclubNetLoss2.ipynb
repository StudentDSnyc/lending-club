{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building a Classifier from Lending Club Data\n",
    "**An end-to-end machine learning example using Pandas and Scikit-Learn** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Ingestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>unemp_LSFT</th>\n",
       "      <th>ilc_mean</th>\n",
       "      <th>ilc_LSFT</th>\n",
       "      <th>gdp_mean</th>\n",
       "      <th>gdp_LSFT</th>\n",
       "      <th>Tbill_mean</th>\n",
       "      <th>Tbill_LSFT</th>\n",
       "      <th>cc_rate</th>\n",
       "      <th>spread</th>\n",
       "      <th>frac_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4200</td>\n",
       "      <td>36</td>\n",
       "      <td>17.27</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000604</td>\n",
       "      <td>30271.15460</td>\n",
       "      <td>72.408845</td>\n",
       "      <td>58206.27465</td>\n",
       "      <td>3.738387</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.536536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>36</td>\n",
       "      <td>19.05</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>25850.40599</td>\n",
       "      <td>73.564980</td>\n",
       "      <td>50555.73682</td>\n",
       "      <td>170.150897</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.958175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6000</td>\n",
       "      <td>36</td>\n",
       "      <td>10.16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>94000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>44084.32570</td>\n",
       "      <td>17.616317</td>\n",
       "      <td>177320.41480</td>\n",
       "      <td>380.902452</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.699753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   funded_amnt  term  int_rate  emp_length  home_ownership  annual_inc  \\\n",
       "0         4200    36     17.27         7.0               1     50000.0   \n",
       "1         2000    36     19.05        15.0               1     81000.0   \n",
       "2         6000    36     10.16         8.0               1     94000.0   \n",
       "\n",
       "   verification_status  purpose    dti  delinq_2yrs    ...      unemp_LSFT  \\\n",
       "0                    1        1  12.17          0.0    ...       -0.000604   \n",
       "1                    1        3  21.42          0.0    ...       -0.000511   \n",
       "2                    1        1  12.77          0.0    ...       -0.000791   \n",
       "\n",
       "      ilc_mean   ilc_LSFT      gdp_mean    gdp_LSFT  Tbill_mean  Tbill_LSFT  \\\n",
       "0  30271.15460  72.408845   58206.27465    3.738387    0.008303    0.000206   \n",
       "1  25850.40599  73.564980   50555.73682  170.150897    0.008303    0.000206   \n",
       "2  44084.32570  17.616317  177320.41480  380.902452    0.008303    0.000206   \n",
       "\n",
       "    cc_rate  spread  frac_loss  \n",
       "0  0.119133   1.808   0.536536  \n",
       "1  0.119133   1.808   0.958175  \n",
       "2  0.119133   1.808   0.699753  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\n",
    "    \"funded_amnt\",\n",
    "    \"term\",\n",
    "    \"int_rate\",             \n",
    "    \"emp_length\",\n",
    "    \"home_ownership\",\n",
    "    \"annual_inc\",\n",
    "    \"verification_status\",   \n",
    "    \"purpose\",\n",
    "    \"dti\",\n",
    "    \"delinq_2yrs\",           \n",
    "    \"inq_last_6mths\",\n",
    "    \"pub_rec\",\n",
    "    \"revol_bal\",\n",
    "    \"revol_util\",\n",
    "    \"open_acc\",\n",
    "    \"unemp\",\n",
    "    \"unemp_LSFT\",\n",
    "    \"ilc_mean\",\n",
    "    \"ilc_LSFT\",\n",
    "    \"gdp_mean\",\n",
    "    \"gdp_LSFT\",\n",
    "    \"Tbill_mean\",\n",
    "    \"Tbill_LSFT\",\n",
    "    \"cc_rate\",\n",
    "    \"spread\",\n",
    "    \"frac_loss\",\n",
    "]\n",
    "\n",
    "Fnames = names[:-1]\n",
    "\n",
    "label = names[-1]\n",
    "\n",
    "# Open up the earlier CSV to determine how many different types ofentries there are in the column 'loan_status'\n",
    "data_with_all_csv_features = pd.read_csv(\"./data/dfd.csv\")\n",
    "full_data = data_with_all_csv_features[names];\n",
    "data = full_data.copy()[names]\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "The very first thing to do is to explore the dataset and see what's inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11440, 26)\n"
     ]
    }
   ],
   "source": [
    "# This line drops any loan data rows with income more than $300K to remove error data\n",
    "#print data.count()\n",
    "print data.shape\n",
    "#data = data.loc[(data['annual_inc'] <= 300000)]\n",
    "##print data.shape[0]\n",
    "#sdata = data.loc[(data['annual_inc'] <= 300000)]\n",
    "#print sdata.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#areas = full_data[['funded_amnt','term','int_rate', 'loan_status']]\n",
    "#scatter_matrix(areas, alpha=0.2, figsize=(18,18), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.set_context(\"poster\")\n",
    "#sns.countplot(x='home_ownership', hue='loan_status', data=full_data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.set_context(\"poster\")\n",
    "#sns.countplot(x='emp_length', hue='loan_status', data=full_data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.set_context(\"poster\")\n",
    "#sns.countplot(x='term', hue='loan_status', data=full_data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.set_context(\"poster\")\n",
    "#sns.countplot(y='purpose', hue='loan_status', data=full_data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.set_context(\"poster\", font_scale=0.8)                                                  \n",
    "#plt.figure(figsize=(15, 15))                                                                                                                                                                                                                                 \n",
    "#plt.ylabel('Loan Originating State')\n",
    "#sns.countplot(y='addr_state', hue='loan_status', data=full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Management \n",
    "\n",
    "In order to organize our data on disk, we'll need to add the following files:\n",
    "\n",
    "- `README.md`: a markdown file containing information about the dataset and attribution. Will be exposed by the `DESCR` attribute.\n",
    "- `meta.json`: a helper file that contains machine readable information about the dataset like `target_names` and `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "meta = {\n",
    "    'target_names': \"frac_loss\",\n",
    "    'feature_names': list(data.columns),\n",
    "    'categorical_features': {\n",
    "        column: list(data[column].unique())\n",
    "        for column in data.columns\n",
    "        if data[column].dtype == 'object'\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('data/lossmeta.json', 'wb') as f:\n",
    "    json.dump(meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a `meta.json` file by inspecting the data frame that we have constructued. The `target_names` column, is just the two unique values in the `data.loan_status` series; by using the `pd.Series.unique` method - we're guarenteed to spot data errors if there are more or less than two values. The `feature_names` is simply the names of all the columns. \n",
    "\n",
    "Then we get tricky &mdash; we want to store the possible values of each categorical field for lookup later, but how do we know which columns are categorical and which are not? Luckily, Pandas has already done an analysis for us, and has stored the column data type, `data[column].dtype`, as either `int64` or `object`. Here I am using a dictionary comprehension to create a dictionary whose keys are the categorical columns, determined by checking the object type and comparing with `object`, and whose values are a list of unique values for that field. \n",
    "\n",
    "Now that we have everything we need stored on disk, we can create a `load_data` function, which will allow us to load the training and test datasets appropriately from disk and store them in a `Bunch`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac_loss\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "def load_data(root='data'):\n",
    "    # Load the meta data from the file \n",
    "    with open(os.path.join(root, 'lossmeta.json'), 'r') as f:\n",
    "        meta = json.load(f) \n",
    "    \n",
    "    names = meta['feature_names']\n",
    "    \n",
    "    # Load the readme information \n",
    "    with open(os.path.join(root, 'README.md'), 'r') as f:\n",
    "        readme = f.read()    \n",
    "    \n",
    "    X = data[Fnames]\n",
    "    \n",
    "    # Remove the target from the categorical features \n",
    "#    meta['categorical_features'].pop(label)\n",
    "    \n",
    "    y = data[label]\n",
    "    \n",
    "#    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size = 0.2,random_state=14)\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size = 0.2,random_state=10)\n",
    "\n",
    "    \n",
    "    # Return the bunch with the appropriate data chunked apart\n",
    "    return Bunch(\n",
    "        #data = train[names[:-1]],\n",
    "        data = X_train,\n",
    "        #target = train[names[-1]], \n",
    "        target = y_train, \n",
    "        #data_test = test[names[:-1]], \n",
    "        data_test = X_test, \n",
    "        #target_test = test[names[-1]], \n",
    "        target_test = y_test, \n",
    "        target_names = meta['target_names'],\n",
    "        feature_names = meta['feature_names'], \n",
    "        categorical_features = meta['categorical_features'], \n",
    "        DESCR = readme,\n",
    "    )\n",
    "\n",
    "dataset = load_data()\n",
    "print meta['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>unemp</th>\n",
       "      <th>unemp_LSFT</th>\n",
       "      <th>ilc_mean</th>\n",
       "      <th>ilc_LSFT</th>\n",
       "      <th>gdp_mean</th>\n",
       "      <th>gdp_LSFT</th>\n",
       "      <th>Tbill_mean</th>\n",
       "      <th>Tbill_LSFT</th>\n",
       "      <th>cc_rate</th>\n",
       "      <th>spread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>7000</td>\n",
       "      <td>36</td>\n",
       "      <td>12.12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>79500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16.69</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>24891.38578</td>\n",
       "      <td>91.431728</td>\n",
       "      <td>46868.98084</td>\n",
       "      <td>137.414688</td>\n",
       "      <td>0.007616</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.119033</td>\n",
       "      <td>1.5585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>6250</td>\n",
       "      <td>36</td>\n",
       "      <td>13.49</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>30388.88130</td>\n",
       "      <td>108.036406</td>\n",
       "      <td>56445.07005</td>\n",
       "      <td>170.316205</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>3.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>11500</td>\n",
       "      <td>36</td>\n",
       "      <td>14.46</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>29766.56507</td>\n",
       "      <td>125.020853</td>\n",
       "      <td>55693.26632</td>\n",
       "      <td>154.483850</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>2.8955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>2400</td>\n",
       "      <td>36</td>\n",
       "      <td>12.69</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>29384.65296</td>\n",
       "      <td>66.155181</td>\n",
       "      <td>57148.16227</td>\n",
       "      <td>178.976390</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>2.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>12000</td>\n",
       "      <td>36</td>\n",
       "      <td>15.31</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>26730.43973</td>\n",
       "      <td>72.676547</td>\n",
       "      <td>53595.91658</td>\n",
       "      <td>122.546268</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.119267</td>\n",
       "      <td>1.4970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      funded_amnt  term  int_rate  emp_length  home_ownership  annual_inc  \\\n",
       "2242         7000    36     12.12         8.0               3     79500.0   \n",
       "7774         6250    36     13.49         5.0               1     86000.0   \n",
       "8304        11500    36     14.46         5.0               3     25200.0   \n",
       "6851         2400    36     12.69         5.0               2     48000.0   \n",
       "2953        12000    36     15.31         7.0               3     76000.0   \n",
       "\n",
       "      verification_status  purpose    dti  delinq_2yrs   ...    unemp  \\\n",
       "2242                    3        2  16.69          2.0   ...    0.104   \n",
       "7774                    3        1  24.20          1.0   ...    0.113   \n",
       "8304                    1        5  13.71          0.0   ...    0.121   \n",
       "6851                    1       11  15.28          0.0   ...    0.071   \n",
       "2953                    2        1   9.92          0.0   ...    0.058   \n",
       "\n",
       "      unemp_LSFT     ilc_mean    ilc_LSFT     gdp_mean    gdp_LSFT  \\\n",
       "2242   -0.001681  24891.38578   91.431728  46868.98084  137.414688   \n",
       "7774   -0.000308  30388.88130  108.036406  56445.07005  170.316205   \n",
       "8304   -0.000198  29766.56507  125.020853  55693.26632  154.483850   \n",
       "6851   -0.000379  29384.65296   66.155181  57148.16227  178.976390   \n",
       "2953   -0.000962  26730.43973   72.676547  53595.91658  122.546268   \n",
       "\n",
       "      Tbill_mean  Tbill_LSFT   cc_rate  spread  \n",
       "2242    0.007616    0.000209  0.119033  1.5585  \n",
       "7774    0.005246    0.000073  0.128900  3.1265  \n",
       "8304    0.005700   -0.000110  0.134367  2.8955  \n",
       "6851    0.005830    0.000191  0.123600  2.0945  \n",
       "2953    0.007457    0.000220  0.119267  1.4970  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary work of the `load_data` function is to locate the appropriate files on disk, given a root directory that's passed in as an argument (if you saved your data in a different directory, you can modify the root to have it look in the right place). The meta data is included with the bunch, and is also used split the train and test datasets into `data` and `target` variables appropriately, such that we can pass them correctly to the Scikit-Learn `fit` and `predict` estimator methods. \n",
    "\n",
    "## Feature Extraction \n",
    "\n",
    "Now that our data management workflow is structured a bit more like Scikit-Learn, we can start to use our data to fit models. Unfortunately, the categorical values themselves are not useful for machine learning; we need a single instance table that contains _numeric values_. In order to extract this from the dataset, we'll have to use Scikit-Learn transformers to transform our input dataset into something that can be fit to a model. In particular, we'll have to do the following:\n",
    "\n",
    "- encode the categorical labels as numeric data \n",
    "- impute missing values with data (or remove)\n",
    "\n",
    "We will explore how to apply these transformations to our dataset, then we will create a feature extraction pipeline that we can use to build a model from the raw input data. This pipeline will apply both the imputer and the label encoders directly in front of our classifier, so that we can ensure that features are extracted appropriately in both the training and test datasets.  \n",
    "\n",
    "### Label Encoding \n",
    "\n",
    "Our first step is to get our data out of the object data type land and into a numeric type, since nearly all operations we'd like to apply to our data are going to rely on numeric types. Luckily, Sckit-Learn does provide a transformer for converting categorical labels into numeric integers: [`sklearn.preprocessing.LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). Unfortunately it can only transform a single vector at a time, so we'll have to adapt it in order to apply it to multiple columns. \n",
    "\n",
    "Like all Scikit-Learn transformers, the `LabelEncoder` has `fit` and `transform` methods (as well as a special all-in-one, `fit_transform` method) that can be used for stateful transformation of a dataset. In the case of the `LabelEncoder`, the `fit` method discovers all unique elements in the given vector, orders them lexicographically, and assigns them an integer value. These values are actually the indices of the elements inside the `LabelEncoder.classes_` attribute, which can also be used to do a reverse lookup of the class name from the integer value. \n",
    "\n",
    "For example, if we were to encode the `home_ownership` column of our dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "ownership = LabelEncoder() \n",
    "ownership.fit(dataset.data.home_ownership)\n",
    "print(ownership.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "purpose = LabelEncoder() \n",
    "purpose.fit(dataset.data.purpose)\n",
    "print(purpose.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this is very useful for a single column, and in fact the `LabelEncoder` really was intended to encode the target variable, not necessarily categorical data expected by the classifiers.\n",
    "\n",
    "In order to create a multicolumn LabelEncoder, we'll have to extend the `TransformerMixin` in Scikit-Learn to create a transformer class of our own, then provide `fit` and `transform` methods that wrap individual `LabelEncoders` for our columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EncodeCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes a specified list of columns or all columns if None. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns=None):\n",
    "        self.columns  = columns \n",
    "        self.encoders = None\n",
    "    \n",
    "    def fit(self, data, target=None):\n",
    "        \"\"\"\n",
    "        Expects a data frame with named columns to encode. \n",
    "        \"\"\"\n",
    "        # Encode all columns if columns is None\n",
    "        if self.columns is None:\n",
    "            self.columns = data.columns \n",
    "        \n",
    "        # Fit a label encoder for each column in the data frame\n",
    "        self.encoders = {\n",
    "            column: LabelEncoder().fit(data[column])\n",
    "            for column in self.columns \n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Uses the encoders to transform a data frame. \n",
    "        \"\"\"\n",
    "        output = data.copy()\n",
    "        for column, encoder in self.encoders.items():\n",
    "            output[column] = encoder.transform(data[column])\n",
    "        \n",
    "        return output\n",
    "\n",
    "encoder = EncodeCategorical(dataset.categorical_features.keys())\n",
    "\n",
    "#data = encoder.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specialized transformer now has the ability to label encode multiple columns in a data frame, saving information about the state of the encoders. It would be trivial to add an `inverse_transform` method that accepts numeric data and converts it to labels, using the `inverse_transform` method of each individual `LabelEncoder` on a per-column basis. \n",
    "\n",
    "### Imputation \n",
    "\n",
    "Scikit-Learn provides a transformer for dealing with missing values at either the column level or at the row level in the `sklearn.preprocessing` library called the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html).\n",
    "\n",
    "The `Imputer` requires information about what missing values are, either an integer or the string, `Nan` for `np.nan` data types, it then requires a strategy for dealing with it. For example, the `Imputer` can fill in the missing values with the mean, median, or most frequent values for each column. If provided an axis argument of 0 then columns that contain only missing data are discarded; if provided an axis argument of 1, then rows which contain only missing values raise an exception. Basic usage of the `Imputer` is as follows:\n",
    "\n",
    "```python\n",
    "imputer = Imputer(missing_values='Nan', strategy='most_frequent')\n",
    "imputer.fit(dataset.data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer \n",
    "\n",
    "class ImputeCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes a specified list of columns or all columns if None. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns \n",
    "        self.imputer = None\n",
    "    \n",
    "    def fit(self, data, target=None):\n",
    "        \"\"\"\n",
    "        Expects a data frame with named columns to impute. \n",
    "        \"\"\"\n",
    "        # Encode all columns if columns is None\n",
    "        if self.columns is None:\n",
    "            self.columns = data.columns \n",
    "        \n",
    "        # Fit an imputer for each column in the data frame\n",
    "        #self.imputer = Imputer(strategy='most_frequent')\n",
    "        self.imputer = Imputer(strategy='mean')\n",
    "        self.imputer.fit(data[self.columns])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Uses the encoders to transform a data frame. \n",
    "        \"\"\"\n",
    "        output = data.copy()\n",
    "        output[self.columns] = self.imputer.transform(output[self.columns])\n",
    "        \n",
    "        return output\n",
    "\n",
    "imputer = ImputeCategorical(Fnames)\n",
    "    \n",
    "#data = imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>unemp_LSFT</th>\n",
       "      <th>ilc_mean</th>\n",
       "      <th>ilc_LSFT</th>\n",
       "      <th>gdp_mean</th>\n",
       "      <th>gdp_LSFT</th>\n",
       "      <th>Tbill_mean</th>\n",
       "      <th>Tbill_LSFT</th>\n",
       "      <th>cc_rate</th>\n",
       "      <th>spread</th>\n",
       "      <th>frac_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4200</td>\n",
       "      <td>36</td>\n",
       "      <td>17.27</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000604</td>\n",
       "      <td>30271.15460</td>\n",
       "      <td>72.408845</td>\n",
       "      <td>58206.27465</td>\n",
       "      <td>3.738387</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.536536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>36</td>\n",
       "      <td>19.05</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>25850.40599</td>\n",
       "      <td>73.564980</td>\n",
       "      <td>50555.73682</td>\n",
       "      <td>170.150897</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.958175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6000</td>\n",
       "      <td>36</td>\n",
       "      <td>10.16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>94000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>44084.32570</td>\n",
       "      <td>17.616317</td>\n",
       "      <td>177320.41480</td>\n",
       "      <td>380.902452</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.699753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4000</td>\n",
       "      <td>36</td>\n",
       "      <td>17.27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>37038.01293</td>\n",
       "      <td>104.073197</td>\n",
       "      <td>70375.20408</td>\n",
       "      <td>213.266500</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.867090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5275</td>\n",
       "      <td>36</td>\n",
       "      <td>15.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>37038.01293</td>\n",
       "      <td>104.073197</td>\n",
       "      <td>70375.20408</td>\n",
       "      <td>213.266500</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.864241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   funded_amnt  term  int_rate  emp_length  home_ownership  annual_inc  \\\n",
       "0         4200    36     17.27         7.0               1     50000.0   \n",
       "1         2000    36     19.05        15.0               1     81000.0   \n",
       "2         6000    36     10.16         8.0               1     94000.0   \n",
       "3         4000    36     17.27         2.0               1     33000.0   \n",
       "4         5275    36     15.80         1.0               3     60000.0   \n",
       "\n",
       "   verification_status  purpose    dti  delinq_2yrs    ...      unemp_LSFT  \\\n",
       "0                    1        1  12.17          0.0    ...       -0.000604   \n",
       "1                    1        3  21.42          0.0    ...       -0.000511   \n",
       "2                    1        1  12.77          0.0    ...       -0.000791   \n",
       "3                    1        2  11.78          1.0    ...       -0.000742   \n",
       "4                    1        2   8.62          0.0    ...       -0.000742   \n",
       "\n",
       "      ilc_mean    ilc_LSFT      gdp_mean    gdp_LSFT  Tbill_mean  Tbill_LSFT  \\\n",
       "0  30271.15460   72.408845   58206.27465    3.738387    0.008303    0.000206   \n",
       "1  25850.40599   73.564980   50555.73682  170.150897    0.008303    0.000206   \n",
       "2  44084.32570   17.616317  177320.41480  380.902452    0.008303    0.000206   \n",
       "3  37038.01293  104.073197   70375.20408  213.266500    0.008303    0.000206   \n",
       "4  37038.01293  104.073197   70375.20408  213.266500    0.008303    0.000206   \n",
       "\n",
       "    cc_rate  spread  frac_loss  \n",
       "0  0.119133   1.808   0.536536  \n",
       "1  0.119133   1.808   0.958175  \n",
       "2  0.119133   1.808   0.699753  \n",
       "3  0.119133   1.808   0.867090  \n",
       "4  0.119133   1.808   0.864241  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom imputer, like the `EncodeCategorical` transformer takes a set of columns to perform imputation on. In this case we only wrap a single `Imputer` as the `Imputer` is multicolumn &mdash; all that's required is to ensure that the correct columns are transformed. \n",
    "\n",
    "I had chosen to do the label encoding first, assuming that because the `Imputer` required numeric values, I'd be able to do the parsing in advance. However, after requiring a custom imputer, I'd say that it's probably best to deal with the missing values early, when they're still a specific value, rather than take a chance. \n",
    "\n",
    "## Model Build \n",
    "\n",
    "To create classifier, we're going to create a [`Pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that uses our feature transformers and ends in an estimator that can do classification. We can then write the entire pipeline object to disk with the `pickle`, allowing us to load it up and use it to make predictions in the future. \n",
    "\n",
    "A pipeline is a step-by-step set of transformers that takes input data and transforms it, until finally passing it to an estimator at the end. Pipelines can be constructed using a named declarative syntax so that they're easy to modify and develop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('encoder', EncodeCategorical(columns=[])), ('imputer', ImputeCategorical(columns=['funded_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'purpose', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec', 'revol_bal', 'revol_util', 'open_acc', 'unem...rue, with_mean=True, with_std=True)), ('classifier', PCA(copy=True, n_components=20, whiten=False))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n",
    "\n",
    "# we need to encode our target data as well. \n",
    "\n",
    "yencode = LabelEncoder().fit(dataset.target)\n",
    "#print yencode\n",
    "\n",
    "# construct the pipeline \n",
    "pca = Pipeline([\n",
    "        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n",
    "        ('imputer', ImputeCategorical(Fnames)), \n",
    "        ('scalar', StandardScaler()),        \n",
    "       ('classifier', PCA(n_components=20))\n",
    "#        ('classifier', PCA())\n",
    "    ])\n",
    "\n",
    "# fit the pipeline \n",
    "pca.fit(dataset.data, yencode.transform(dataset.target))\n",
    "#print dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14.97  26.65  35.67  42.9   48.65  53.52  58.06  62.35  66.51  70.44\n",
      "  74.25  77.77  81.21  84.1   86.68  89.21  91.21  93.07  94.74  96.32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1489de48>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAECCAYAAADw0Rw8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkQkyinoRUSinIJeRCTKKehFRKLc/wdvNqPhBmNazQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe335048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.named_steps['classifier'].explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.named_steps['classifier'].explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "print var1\n",
    "plt.plot(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Fits a linear Regression model to data and makes predictions about the probability of a categorical event "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('encoder', EncodeCategorical(columns=[])), ('imputer', ImputeCategorical(columns=['funded_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'purpose', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec', 'revol_bal', 'revol_util', 'open_acc', 'unem...rue)), ('classifier', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# we need to encode our target data as well. \n",
    "\n",
    "yencode = LabelEncoder().fit(dataset.target)\n",
    "\n",
    "#normalizer = Normalizer(copy=False)\n",
    "\n",
    "# construct the pipeline \n",
    "lr = Pipeline([\n",
    "        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n",
    "        ('imputer', ImputeCategorical(Fnames)), \n",
    "        ('scalar', StandardScaler()),\n",
    "        #('normalizer', Normalizer(copy=False)),\n",
    "        ('classifier', LinearRegression())\n",
    "    ])\n",
    "\n",
    "# fit the pipeline \n",
    "lr.fit(dataset.data, yencode.transform(dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATSET Mean Squared Error: 21346781.320\n",
      "Coefficient of Determination: -341738824.015\n",
      "Variance score: -341738824.02\n",
      "TRAIN DATASET Mean Squared Error: 21196942.600\n",
      "Coefficient of Determination: -337232855.601\n",
      "FULL DATASET Mean Squared Error: 21226910.344\n",
      "Coefficient of Determination: -338125883.723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TEST UNDER TEST DATASET\n",
    "\n",
    "# encode test targets\n",
    "#y_true = yencode.transform(dataset.target_test)\n",
    "y_true = dataset.target_test\n",
    "# use the model to get the predicted value\n",
    "y_pred = lr.predict(dataset.data_test)\n",
    "# execute classification report \n",
    "#print classification_report(y_true, y_pred, target_names=dataset.target_names)\n",
    "# Evaluate fit of the model\n",
    "print \"TEST DATSET Mean Squared Error: %0.3f\" % mse(y_true, y_pred)\n",
    "print \"Coefficient of Determination: %0.3f\" % r2_score(y_true, y_pred)\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % lr.score(dataset.data_test, dataset.target_test))\n",
    "\n",
    "#TEST UNDER TRAIN DATASET\n",
    "# encode test targets\n",
    "#y_true = yencode.transform(dataset.target)\n",
    "y_true = dataset.target\n",
    "# use the model to get the predicted value\n",
    "y_pred = lr.predict(dataset.data)\n",
    "# execute classification report \n",
    "#print classification_report(y_true, y_pred, target_names=dataset.target_names)\n",
    "print \"TRAIN DATASET Mean Squared Error: %0.3f\" % mse(y_true, y_pred)\n",
    "print \"Coefficient of Determination: %0.3f\" % r2_score(y_true, y_pred)\n",
    "\n",
    "#TEST UNDER FULL DATASET\n",
    "\n",
    "#lr.fit(full_data[Fnames], yencode.transform(full_data[label]))\n",
    "# encode test targets\n",
    "#y_true = yencode.transform(full_data[label])\n",
    "y_true = full_data[label]\n",
    "# use the model to get the predicted value\n",
    "y_pred = lr.predict(full_data[Fnames])\n",
    "# execute classification report \n",
    "#print classification_report(y_true, y_pred, target_names=dataset.target_names)\n",
    "print \"FULL DATASET Mean Squared Error: %0.3f\" % mse(y_true, y_pred)\n",
    "print \"Coefficient of Determination: %0.3f\" % r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet\n",
    "\n",
    "ElasticNet is a linear combination of L1 and L2 regularization, meaning it combines Ridge and LASSO and essentially splits the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('encoder', EncodeCategorical(columns=[])), ('imputer', ImputeCategorical(columns=['funded_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'purpose', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec', 'revol_bal', 'revol_util', 'open_acc', 'unem...alse, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we need to encode our target data as well. \n",
    "yencode = LabelEncoder().fit(dataset.target)\n",
    "\n",
    "\n",
    "# construct the pipeline \n",
    "lelastic = Pipeline([\n",
    "        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n",
    "        ('imputer', ImputeCategorical(Fnames)), \n",
    "        ('scalar', StandardScaler()),\n",
    "        ('classifier', ElasticNet(alpha=0.01, l1_ratio =0.1))\n",
    "    ])\n",
    "\n",
    "\n",
    "# fit the pipeline \n",
    "lelastic.fit(dataset.data, yencode.transform(dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 21341030.998\n",
      "Coefficient of Determination: -341646767.595\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# encode test targets\n",
    "#y_true = yencode.transform(dataset.target_test)\n",
    "y_true = dataset.target_test\n",
    "\n",
    "# use the model to get the predicted value\n",
    "y_pred = lelastic.predict(dataset.data_test)\n",
    "\n",
    "#print dataset.data_test.head(1)\n",
    "\n",
    "# execute classification report \n",
    "#print classification_report(y_true, y_pred, target_names=dataset.target_names)\n",
    "#print classification_report(y_true, y_pred, target_names=[\"Default\",\"Fully Paid\"])\n",
    "# Evaluate fit of the model\n",
    "print \"Mean Squared Error: %0.3f\" % mse(y_true, y_pred)\n",
    "print \"Coefficient of Determination: %0.3f\" % r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -35.73187834  226.37923797  303.2448814  -103.12196327   55.39987937\n",
      "  -12.79386002   56.65913151  126.68824753   78.47378238   10.43284558\n",
      "  222.68192694  -74.98503679  104.25645707  -96.33592355 -183.86741191\n",
      "   78.3055384    97.34130382  -18.6875633   -20.94644145  -23.69878825\n",
      "   34.63908193  -66.95822996 -129.3747991  -139.68525919   72.80293601]\n",
      "Linear model: -35.732 * funded_amnt + 226.379 * term + 303.245 * int_rate + -103.122 * emp_length + 55.4 * home_ownership + -12.794 * annual_inc + 56.659 * verification_status + 126.688 * purpose + 78.474 * dti + 10.433 * delinq_2yrs + 222.682 * inq_last_6mths + -74.985 * pub_rec + 104.256 * revol_bal + -96.336 * revol_util + -183.867 * open_acc + 78.306 * unemp + 97.341 * unemp_LSFT + -18.688 * ilc_mean + -20.946 * ilc_LSFT + -23.699 * gdp_mean + 34.639 * gdp_LSFT + -66.958 * Tbill_mean + -129.375 * Tbill_LSFT + -139.685 * cc_rate + 72.803 * spread\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    "\n",
    "coefs = lelastic.named_steps['classifier'].coef_\n",
    "print coefs\n",
    "print \"Linear model:\", pretty_print_linear(coefs, Fnames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbor\n",
    "\n",
    "Makes predictions by locating similar instances via a similarity function or distance and averaging the majority of the most similar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 23096055.885\n",
      "Coefficient of Determination: -369742813.270\n"
     ]
    }
   ],
   "source": [
    "# K- Neighbor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# we need to encode our target data as well. \n",
    "\n",
    "yencode = LabelEncoder().fit(dataset.target)\n",
    "\n",
    "# construct the pipeline \n",
    "kn = Pipeline([\n",
    "        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n",
    "        ('imputer', ImputeCategorical(Fnames)), \n",
    "        ('scalar', StandardScaler()),\n",
    "        ('classifier', KNeighborsRegressor(n_neighbors=3))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Next split up the data with the 'train test split' method in the Cross Validation module\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "# ...and then run the 'fit' method to build a model\n",
    "kn.fit(dataset.data, yencode.transform(dataset.target))\n",
    "\n",
    "\n",
    "#y_true = yencode.transform(dataset.target_test)\n",
    "y_true = dataset.target_test\n",
    "\n",
    "y_pred  = kn.predict(dataset.data_test)\n",
    "\n",
    "#print classification_report(y_true, predicted, target_names=dataset.target_names)\n",
    "\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print \"Mean Squared Error: %0.3f\" % mse(y_true, y_pred)\n",
    "print \"Coefficient of Determination: %0.3f\" % r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVRs\n",
    "\n",
    "Support Vector Regression (SVR) uses points in transformed problem space that separates the classes into groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.062\n",
      "Coefficient of Determination: 0.013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# we need to encode our target data as well. \n",
    "\n",
    "#yencode = LabelEncoder().fit(dataset.target)\n",
    "yencode = dataset.target\n",
    "\n",
    "# construct the pipeline \n",
    "svr = Pipeline([\n",
    "        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n",
    "        ('imputer', ImputeCategorical(Fnames)), \n",
    "        ('scalar', StandardScaler()),\n",
    "        ('classifier', SVR(kernel='linear'))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Next split up the data with the 'train test split' method in the Cross Validation module\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "# ...and then run the 'fit' method to build a model\n",
    "svr.fit(dataset.data, dataset.target)\n",
    "\n",
    "\n",
    "y_true = dataset.target_test\n",
    "\n",
    "y_pred  = svr.predict(dataset.data_test)\n",
    "\n",
    "\n",
    "\n",
    "#print classification_report(y_true, predicted, target_names=dataset.target_names)\n",
    "\n",
    "\n",
    "#kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "\n",
    "#for kernel in kernels:\n",
    "#    if kernel != 'poly':\n",
    "#        model      = SVC(kernel=kernel)\n",
    "#    else:\n",
    "#        model      = SVC(kernel=kernel, degree=3)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print \"Mean Squared Error: %0.3f\" % mse(y_true, y_pred)\n",
    "print \"Coefficient of Determination: %0.3f\" % r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242     0.098197\n",
      "7774     0.736762\n",
      "8304     0.880838\n",
      "6851     0.037479\n",
      "2953     0.355358\n",
      "8405     0.526315\n",
      "6571     0.702082\n",
      "8516     0.738103\n",
      "9746     0.822114\n",
      "10726    0.720508\n",
      "5441     0.426134\n",
      "10126    0.809698\n",
      "4976     0.351530\n",
      "5779     0.094443\n",
      "9463     0.066560\n",
      "3914     0.772281\n",
      "11015    0.152887\n",
      "601      0.890735\n",
      "8571     0.578622\n",
      "6349     0.558252\n",
      "4539     0.683898\n",
      "11411    0.860498\n",
      "7214     0.637857\n",
      "166      0.216050\n",
      "5744     0.219758\n",
      "7683     0.904920\n",
      "1810     0.732150\n",
      "285      0.726775\n",
      "1619     0.482748\n",
      "8518     0.167340\n",
      "           ...   \n",
      "7477     0.191006\n",
      "11033    0.100012\n",
      "5149     0.915258\n",
      "10729    0.642052\n",
      "7336     0.468858\n",
      "4342     0.651031\n",
      "9162     0.291142\n",
      "10781    0.586733\n",
      "612      0.916730\n",
      "3810     0.824229\n",
      "5489     0.342480\n",
      "9506     0.493511\n",
      "2330     0.885597\n",
      "60       0.339127\n",
      "4861     0.545675\n",
      "10477    0.649450\n",
      "10761    0.397398\n",
      "2467     0.568351\n",
      "4299     0.512640\n",
      "10643    0.757804\n",
      "10474    0.651635\n",
      "7633     0.415262\n",
      "7204     0.154404\n",
      "112      0.566682\n",
      "3340     0.630711\n",
      "2165     0.411534\n",
      "573      0.217381\n",
      "1696     0.288145\n",
      "11209    0.977077\n",
      "1041     0.517690\n",
      "Name: frac_loss, dtype: float64\n",
      "[ 0.60628913  0.66080075  0.67168474 ...,  0.6354122   0.61098804\n",
      "  0.57300448]\n"
     ]
    }
   ],
   "source": [
    "print y_true\n",
    "print y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
